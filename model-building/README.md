# Model Building

## Videos

- [Official PyTorch Documentary: Powering the AI Revolution](https://www.youtube.com/watch?v=rgP_LBtaUEc)
- [Cursor Team: Future of Programming with AI | Lex Fridman Podcast #447](https://www.youtube.com/watch?v=oFfVt3S51T4)

### How models work

- [How to Create a Neural Network (and Train it to Identify Doodles)](https://www.youtube.com/watch?v=hfMk-kjRv4c)
- [Neural networks by 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [How Transformer LLMs Work](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)
- [The Ultra-Scale Playbook: Training LLMs on GPU Clusters](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
- [FineWeb: decanting the web for the finest text data at scale](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
- [Most devs don't understand how LLM tokens work](https://www.youtube.com/watch?v=nKSk_TiR8YA)
- [The Big LLM Architecture Comparison](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison)
- [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)

### Mixture Of Experts

- [Mixture of Experts (MoE) Introduction](https://www.youtube.com/watch?v=v7U21meXd6Y)
- [The MoE 101 Guide](https://www.cerebras.ai/moe-guide)
- [A Visual Guide to Mixture of Experts (MoE)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

### Reasoning Models

- [Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)

## Performance

- [1.5x Faster MoE Training with Custom MXFP8 Kernels](https://cursor.com/blog/kernels)

## Chat Template

- [Chat Templates](https://huggingface.co/docs/transformers/en/chat_templating)

## Numbers and Formats

- [What’s MXFP4? The 4-Bit Secret Powering OpenAI’s GPT‑OSS Models on Modest Hardware](https://huggingface.co/blog/RakshitAralimatti/learn-ai-with-me)
